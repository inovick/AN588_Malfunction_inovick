---
title: "inovick_final_hw_code_04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Challenge 1
## Write a simple R function that can perform one- or two-sample Z-tests for proportion data
# I had originally gotten the format for the code from typing ?prop.test in the console, as well as looking at the modules (specifically 10). But I had a lot of trouble understanding how to set up this function using the given parameters, specifically the if/then statements and how those corresponded to the variables. I got this updated correct code from my peer commenter, Frank Short. This was really helpful to me because I couldn't conceptualize the order of the values, and I also had a lot of extranneous code and information. This tests whether p2 and n2 are null, and a conditional if statement to read out a warning if the assumptions of a normal distribution are not met.
```{r}
z.prop.test <- function(p1, n1, p2 = NULL, n2 = NULL, p0, alpha = 0.05) {
   
        if(is.null(p2) | is.null(n2)) {
            m <- mean(p1)
            s <- sd(p1)
            t <- t.test(x = p1, mu = p0, alternative = "two.sided")
            ci <- t$conf.int
            t
            if(n1* m <5 & n1*(1- m)<5){
              print("Warning: Sample may not be of a normal distribution.") & print(t)
            }
              else {
                print(t)
              }
            }
            
        
      
        if(!is.null(p2) | !is.null(n2)) {
           pstar <- (sum(p1) + sum(p2))/(length(p1) + length(p2))
           phat1 <- mean(p1)
          phat2 <- mean(p2)
          z <- (phat2 - phat1)/sqrt((pstar * (1 - pstar)) * (1/length(p1) + 1/length(p2)))
          p <- 1 - pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)
          pt <- prop.test(x = c(sum(p2), sum(p1)), n = c(length(p2), length(p1)), alternative = "two.sided", correct = FALSE)
          z
          p
          pt
          if(n1* phat1 <5 & n1*(1- phat1)<5 | n2* phat2 <5 & n1*(1- phat2)<5){
              print("Warning: Sample may not be of a normal distribution.") & print(t)
            }
              else {
                print(pt)
              }
            }
        }
```

## Using the new z.prop.test function on a one sample t-test:
# I had originally thought I needed to make an exemplar population with 1's and 0's like in the module, but it seems like I can just make my own dataset instead, so I tried that and it seemed to work
```{r}
p1 <- c(0.7, 0.75, 0.3, 0.5, 0.9, 0.8, 0.7, 0.77, 0.73, 0.77, 0.72, 0.7, 0.75, 0.71, 0.74, 0.73, 0.66, 0.77, 0.75, 0.71, 0.7, 0.7, 0.78, 0.7, 0.6, 0.7, 0.8, 0.77, 0.76, 0.75, 0.71, 0.81, 0.44, 0.55, 0.7, 0.8, 0.28, 0.6, 0.65, 0.35)
n1 <- 40
p0 <- 0.5 
z.prop.test(p1 = p1, n1 = n1, p0 = p0)

#Getting the mean and standard deviation to use in the two sample t-test:
mean(p1)
sd(p1)
```

## Using the new z.prop.test function on a two sample t-test:
# I used the same dataset of n=40 from the one sample t-test to do the two sample
```{r}
p1 <- rnorm(40, mean = 0.68, sd = 0.14)
n1 <- 40
p0 <- 0.5 
p2 <- rnorm(40, mean = 0.3, sd = 0.4)
n2 <- 40
z.prop.test(p1 = p1, n1 = n1, p0 = p0, p2 = p2, n2 = n2)
```

### Challenge 2
## The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size)
# Install and load packages:
```{r}
#Load necessary packages
library(curl)
library(ggplot2)
library(manipulate)
install.packages("gridExtra")
library(gridExtra)
install.packages("lmodel2")
library(lmodel2)
library(tidyr)
```

#Load data, fitting a regression model to predict longevity
```{r}
#Load the data using curl command
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```
##Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()
#I originally did with both lm and another messier way without axes labels and using the code from module 12, determining the slope by hand instead of using lm. I changed to just using the lm function and geom_point.
```{r}
m <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)
m
names(m)
m$coefficients

lblm <- ggplot(data = d, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m))
lblm<- lblm + geom_point() 
lbllm<- lblm + geom_smooth(method = "lm", formula = y ~ x) + xlab("Mean Species Brain Size (g)") + ylab("Max Species Longevity (months)") + theme_classic()
lblm


loglbl<- lm(log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean), data = d)
loglbl

loglblm <- ggplot(data = d, aes(x = log(Brain_Size_Species_Mean), y = log(MaxLongevity_m)))
loglblm<- loglblm + geom_point() 
loglblm<- loglblm + geom_smooth(method = "lm", formula = y ~ x) + xlab("Log Mean Species Brain Size (g)") + ylab("Log Max Species Longevity (months)") + theme_classic()
loglblm
```
##Intercept is 248.952. This means that when the x=0, brain size would be 248.952 g. I need a little more info on how to interpret this

#Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.
```{r}
summary(lbModel)
ci <- confint(m, level = 0.90)
ci
summary(loglbl)
ci <- confint(loglbl, level = 0.90)
ci

#Log transformed data is a better graph because it cuts out extra and fits it nicely
```
##I found this code on stack exchange to put a legend displaying the equation on the graph. None of my peer commentary group added a legend on their graph, so I may have misinterpreted the directions, but I will leave the code in because it did indeed create a figure legend and I may want to use it later:
```{r}
#Made the variables into a dataframe first
df <- data.frame(x = d$MaxLongevity_m, y = d$Brain_Size_Species_Mean)


lm_eqn <- function(df){
    m <- lm(y ~ x, df);
    eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2, 
         list(a = format(unname(coef(m)[1]), digits = 2),
              b = format(unname(coef(m)[2]), digits = 2),
             r2 = format(summary(m)$r.squared, digits = 3)))
    as.character(as.expression(eq));
}
p1 <- g + geom_text(x = 200, y = 400, label = lm_eqn(df), parse = TRUE)
p1
```

##Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.
#I needed help for this because I didn't completely understand what was being asked. I think I understand a little better, but I hope we can go over this in class together!
```{r}
h_hat <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean))
df <- data.frame(cbind(d$Brain_Size_Species_Mean, d$MaxLongevity_m, h_hat))
names(df) <- c("Mean Species Brain Size", "Max Longevity", "yhat")
head(df)
ci <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean),interval = "confidence",
    level = 0.95)
ci
df <- cbind(df, ci)
names(df) <- c("MeanSpeciesBrainSize", "MaxLongevity", "yhat", "CIfit", "CIlwr", "CIupr")
head(df)
df
```
## Figuring out the best slope-- identifying the point estimate of the slope
```{r}
slope.test <- function(beta1) {
    g <- ggplot(data = d, aes(x = d$Brain_Size_Species_Mean, y = MaxLongevity_m))
    g <- g + geom_point()
    g <- g + geom_abline(intercept = 0, slope = beta1, size = 1, colour = "blue",
        alpha = 1/2)
    ols <- sum((y - beta1 * x)^2)
    g <- g + ggtitle(paste("Slope = ", beta1, "\nSum of Squared Deviations = ",
        round(ols, 3)))
    g
}
#Using manipulate. It just gives me a gray square again. Why??
manipulate(slope.test(beta1), beta1 = slider(-1, 1, initial = 0, step = 0.005))
```
#Confidence interval
```{r}
ci <- confint(m, level = 0.90) # using the results of lm()
ci
```
```

## Peer Commentary - NN
+ What you learned from running their Original Homework Code that helped improve your own code.
My biggest takeaway from your code is the value in consulting external resources like StackExchange since you were able to figure out how to do things (e.g. adding legends) that I didn't even know how to approach. I also appreciate the "stream of consciousness" that's throughout your code since it makes it clear to the viewer why you chose to do certain things. 

+ What you did in your own code that might help to improve theirs.
Our codes are pretty similar but I'd say the only difference is our methods of commenting. I personally find it easier to follow if the comments are separate from the code so when the file is knitted they aren't intermingled with the code - however, if this isn't your preference I think your code looks good! 

+ What challenges, if any, you both faced in your code that could not be helped by comparison.
I think we're both confused with the second half of challenge 2 and that can only be helped by consulting the modules and other online resources. 

+ Whether the annotation/commenting on your peer’s Original Homework Code is readable and interpretable to you, and if not then how it could be improved.
For the most part I was able to follow the annotations and comments in your code, I like how you included code that wasn't necessarily successful too since that shows the breadth of your attempts and the different methods that you considered for answering the various questions. 

Overall, good job! This has been the hardest assignment for me and I think your code looks great. 

